# Data loader configuration
data:
  dataset: "amazon"
  batch_size: 256
  normalize_data: False
  category: "beauty"

# Model configuration
model:
  input_dimension: 768 # computed automatically later
  hidden_dimension: [256, 512, 256] # also the first dimension here
  latent_dimension: 256
  num_codebook_layers: 3
  codebook_clusters: 256
  commitment_weight: 0.25

# Training configuration
train:
  learning_rate: 1e-3
  weight_decay: 1e-4
  num_epochs: 200

# Additional configuration
general:
  use_wandb: True
  wandb_project: "seamantic_id_amazon"
  wandb_entity: "justin-hangoebl-master-thesis"
  save_model: True
